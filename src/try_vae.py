# -*- coding: utf-8 -*-
"""Try VAE.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1aeiL8NnJygfIQxwR96p3_Uw_rKOTkwyt
"""



# Commented out IPython magic to ensure Python compatibility.
# %cd /content/drive/MyDrive/0Thiland Coordination

!pip install geopandas
!pip install wget
!pip install rioxarray



import wget
import pickle
import pandas as pd

import numpy as np
import geopandas as gpd

import pandas as pd
import numpy as np
import pickle
import os
import tarfile
import xarray
from matplotlib import pyplot as plt
from sklearn import  metrics
from scipy.cluster.hierarchy import dendrogram

import rioxarray as xr
from sklearn.manifold import TSNE
import matplotlib.pyplot as plt

"""## Utils

"""

def cluster_with_kmeans(df,n=5):
  kmeans = KMeans(n_clusters=n,max_iter=10000,n_init=20).fit(df)

  clusters_dict={}
  l=kmeans.labels_
  for i,c in enumerate(l):
    if c in clusters_dict:
      clusters_dict[c].append(i)
    else:
      clusters_dict[c]=[i,]

  clusters_dict_names={}
  for k in clusters_dict:
    clusters_dict_names[k]=[]
    for city_idx in clusters_dict[k]:
      clusters_dict_names[k].append(idx_name[city_idx])
  
  return clusters_dict_names,l


def agg_clustering(df,n=5,link='ward'):
  AC=AgglomerativeClustering(n_clusters=n,linkage=link).fit(df)
  clusters_dict={}
  l=AC.labels_
  for i,c in enumerate(l):
    if c in clusters_dict:
      clusters_dict[c].append(i)
    else:
      clusters_dict[c]=[i,]

  clusters_dict_names={}
  for k in clusters_dict:
    clusters_dict_names[k]=[]
    for city_idx in clusters_dict[k]:
      clusters_dict_names[k].append(idx_name[city_idx])
  
  return AC,clusters_dict_names,l


def viz_clusters(df,clusters_dict_names,inc_names=False):

  plt.figure(figsize=(10,10))  
  for c in clusters_dict_names.keys():
    cities=clusters_dict_names[c]
    inds=[]
    for city in cities:
      inds.append(name_idx[city])
    

    
    pts=df[inds]
    plt.scatter(pts[:,0],pts[:,1])
    _avg=np.mean(pts,axis=0)
    plt.annotate(c,(_avg[0],_avg[1]))


  v=[clusters_dict_names[k][0] for k in clusters_dict_names.keys()]  
  plt.legend(v)  
  plt.show()

  if inc_names:
    plt.figure(figsize=(30,20))
    for c in clusters_dict_names.keys():
      cities=clusters_dict_names[c]
      inds=[]
      for city in cities:
        inds.append(name_idx[city])
      

      
      pts=df[inds]
      plt.scatter(pts[:,0],pts[:,1])
      for ind in inds:
        plt.text(df[ind,0],df[ind,1],idx_name[ind],fontsize=15)
    
    plt.show()

    

def plot_dendrogram(model, **kwargs):
    # Create linkage matrix and then plot the dendrogram

    # create the counts of samples under each node
    counts = np.zeros(model.children_.shape[0])
    n_samples = len(model.labels_)
    for i, merge in enumerate(model.children_):
        current_count = 0
        for child_idx in merge:
            if child_idx < n_samples:
                current_count += 1  # leaf node
            else:
                current_count += counts[child_idx - n_samples]
        counts[i] = current_count

    linkage_matrix = np.column_stack([model.children_, model.distances_,
                                      counts]).astype(float)

    # Plot the corresponding dendrogram
    dendrogram(linkage_matrix, **kwargs)

def evaluate_clustering(df,labels):
  return metrics.silhouette_score(df, labels, metric='euclidean'),metrics.calinski_harabasz_score(df, labels)

def get_clus_dist(dict_clus):
  c_num={}
  n=len(dict_clus)
  for k in dict_clus:
    c_num[k]=len(dict_clus[k])
  
  plt.bar(list(range(n)),c_num)
  return c_num

# new_all_cities_file = os.path.join("GHS_FUA_UCDB2015_GLOBE_R2019A_54009_1K_V1_0", "GHS_FUA_UCDB2015_GLOBE_R2019A_54009_1K_V1_0.gpkg")
# new_all_cities_gdf = gpd.read_file(new_all_cities_file)
# new_all_cities_gdf_filterd_2=new_all_cities_gdf[new_all_cities_gdf['FUA_p_2015'] > 5000000 ]

"""## Gather All data"""

new_all_cities_file = os.path.join("GHS_FUA_UCDB2015_GLOBE_R2019A_54009_1K_V1_0", "GHS_FUA_UCDB2015_GLOBE_R2019A_54009_1K_V1_0.gpkg")
new_all_cities_gdf = gpd.read_file(new_all_cities_file)
new_all_cities_gdf_filterd_2=new_all_cities_gdf
new_all_cities_gdf_filterd_2.drop_duplicates('eFUA_name',inplace=True)
new_all_cities_gdf_filterd_2.sort_values('FUA_p_2015',ascending=False,inplace=True)

## Handle area dict
area_dict=pickle.load(open('/content/drive/MyDrive/0Thiland Coordination/Built Up data/all_area_dict.pickle','rb'))
area_dict_re={}
for k in area_dict:
  area_dict_re[k]=np.array(area_dict[k][0][1:])/255.0
area_df=pd.DataFrame.from_dict(area_dict_re,orient='index',columns=['1990_area','2000_area','2015_area','water'])
area_df.shape

## Handle population
pop_df=pickle.load(open('/content/drive/MyDrive/0Thiland Coordination/Population/population_df_all.pickle','rb'))
pop_df.loc[['Tokyo','Shanghai','Quezon City [Manila]'],:]

## Handle city light
dgf=gpd.read_file('/content/drive/MyDrive/0Thiland Coordination/CSV Datasets/cities_pop_light.geojson')
clnames=['eFUA_name','geometry','CL2014',	'CL2013',	'CL2018',	'CL2016',	'CL2019',	'CL2020',	'CL2015',	'CL2017',	'CL2012']
ld=dgf.loc[:,clnames]
ld.set_index(ld.eFUA_name,inplace=True)
ld.drop_duplicates('eFUA_name',inplace=True)
subdf=ld.loc[new_all_cities_gdf_filterd_2.eFUA_name,:]
subdf.drop(['eFUA_name','geometry'],axis=1,inplace=True)
cl_df=subdf

cl_df.shape

## Handle NDVI
ndvi_dict=pickle.load(open('/content/drive/MyDrive/0Thiland Coordination/NDVI/df_allvars.pickle','rb'))
top300_2015=pickle.load(open('/content/drive/MyDrive/0Thiland Coordination/NDVI/top300_2015.pickle','rb'))
for k in top300_2015:
  
  if k in ndvi_dict:
    if len(ndvi_dict[k])<15:
      ndvi_dict[k][-3:]=top300_2015[k][:3]
      for i in range(2):
        ndvi_dict[k].append(top300_2015[k][i+3])

ndvi_dict['Chicago']

## Handle NAVI 
ndvi_df=pd.DataFrame.from_dict(ndvi_dict, orient='index',columns=['1990_mean','1990_min','1990_max','1990_median','1990_std','2000_mean','2000_min','2000_max','2000_median','2000_std','2015_mean','2015_min','2015_max','2015_median','2015_std'])
ndvi_df.loc[['New York','Boston','Tokyo'],:]#.loc[['Tokyo','Shanghai','Quezon City [Manila]'],:]

ndvi_df.dropna(inplace=True)
ndvi_df.shape

ndvi_df.loc[['Boston','Tokyo'],:]

## Concatenate all
# pop_df_sub=pop_df.loc[list(ndvi_df.index),:]
ndind=list(ndvi_df.index)
popind=list(pop_df.index)
areaind=list(area_df.index)
clind=list(cl_df.index)
jointind=[]
for ind in ndind:
  if ind in popind and ind in areaind and ind in clind:
    jointind.append(ind)

len(jointind)

ndvi_df['1990_range']=ndvi_df['1990_max']-ndvi_df['1990_min']
ndvi_df['2000_range']=ndvi_df['2000_max']-ndvi_df['2000_min']
ndvi_df['2015_range']=ndvi_df['2015_max']-ndvi_df['2015_min']
ndvi_df.drop(['1990_max','2000_max','2015_max','1990_min','2000_min','2015_min'],axis=1,inplace=True)

## Concatenate all
pop_df_sub=pop_df.loc[jointind,:]
area_df_sub=area_df.loc[jointind,:]
ndvi_df_sub=ndvi_df.loc[jointind,:]
cl_df_sub=cl_df.loc[jointind,:]
all_df=pd.concat([area_df_sub,pop_df_sub,cl_df_sub,ndvi_df_sub],axis=1)

## SAve data up to NDVI
pickle.dump(all_df,open('/content/drive/MyDrive/0Thiland Coordination/all_data_8k.pickle','wb'))



# load data up to ndvi
all_data_sofar=pickle.load(open('/content/drive/MyDrive/0Thiland Coordination/all_data_8k.pickle','rb'))
all_data_sofar.shape

## Handle Soil Temp.

st_df=pd.read_csv('/content/drive/MyDrive/0Thiland Coordination/CSV Datasets/soil_temp.csv')
st_df.drop_duplicates('eFUA_name',inplace=True)
st_df.drop(['eFUA_ID','Cntry_name'],inplace=True,axis=1)
st_df.set_index('eFUA_name',inplace=True)
st_filtered=st_df.loc[all_df.index,:]
all_df_with_st=pd.concat([all_df,st_filtered],axis=1)
all_df_with_st.shape

## SAve data up to soil temp
pickle.dump(all_df_with_st,open('/content/drive/MyDrive/0Thiland Coordination/all_data_with_st_nonvi.pickle','wb'))

"""### Get City Light Data."""

wget.download('https://mit.enterprise.slack.com/files/W010AAUKX0V/F0264JV3X34/cities_pop_light.geojson')

dgf.columns

dgf=gpd.read_file('/content/drive/MyDrive/0Thiland Coordination/cities_pop_light.geojson')

dgf.drop_duplicates(['eFUA_name','geometry'],inplace=True)

clnames=['eFUA_name','geometry','CL2014',	'CL2013',	'CL2018',	'CL2016',	'CL2019',	'CL2020',	'CL2015',	'CL2017',	'CL2012']
ld=dgf.loc[:,clnames]

alldf=pickle.load(open('/content/drive/MyDrive/0Thiland Coordination/pickle_files/all_df.pickle','rb'))

inds=alldf.index
ld.index=ld.eFUA_name
ld_100=ld.loc[alldf.index,:]
ld_100.drop_duplicates('eFUA_name',inplace=True)
all_df_jn=alldf.loc[ld_100.index,:]

all_df_jn['name']=all_df_jn.index.values

all_df_jn.drop_duplicates('name',inplace=True)



df_all_cl=pd.concat([all_df_jn,ld_100],axis=1)

df_all_cl.drop(['geometry','eFUA_name','name'],axis=1,inplace=True)

df_all_cl.drop(df_all_cl.columns[5:8],axis=1,inplace=True)

pickle.dump(df_all_cl,open('/content/drive/MyDrive/0Thiland Coordination/pickle_files/nondvi_withcity_lights.pkl','wb'))

df_all_cl



"""## try without vae"""

df_nondvi=pd.read_csv('/content/drive/MyDrive/0Thiland Coordination/CSV Datasets/Soc_econ_data_4paper.csv')
df_nondvi.set_index('eFUA_name',inplace=True)
X=df_nondvi.values
rows,cols=X.shape

X_scaled=((df_nondvi-df_nondvi.mean())/df_nondvi.std()).values
X_scaled.mean(axis=0)

from sklearn.cluster import AgglomerativeClustering,KMeans

ag=AgglomerativeClustering(n_clusters=8)
ag.fit(X_scaled)

cls_scr_sc={}
cls_scr_r={}

for n_c in range(3,10):
  ag=AgglomerativeClustering(n_clusters=n_c,linkage='ward')
  ag.fit(X_scaled)
  r=evaluate_clustering(X_scaled,ag.labels_)
  print(r)
  x,y=np.unique(ag.labels_,return_counts=True)
  print(y)
  cls_scr_sc[n_c]=r[-1]
  cls_scr_r[n_c]=r[0]

x,y=np.unique(ag.labels_,return_counts=True)

# cluster dis
plt.bar([0,1,2,3],[  58,3156,4846,534])

from sklearn.manifold import TSNE
ts=TSNE(2)
X_2d=ts.fit_transform(X_scaled)

import matplotlib.pyplot as plt
plt.scatter(X_2d[:,0],X_2d[:,1])

idx_name={i:name for i,name in enumerate(df_nondvi.index)}
name_idx={name:i for i,name in enumerate(df_nondvi.index)}

# plot dendogram
# code from here :https://stackabuse.com/hierarchical-clustering-with-python-and-scikit-learn
from scipy.cluster.hierarchy import dendrogram, linkage

linked = linkage(X_enc,'ward')

labelList = range(len(X_enc))

plt.figure(figsize=(10, 12))
dendrogram(linked,
            orientation='top',
            labels=labelList,
            distance_sort='descending',
            show_leaf_counts=True)
plt.show()

ag_model,clus_names,clus,=agg_clustering(X_scaled,4)

viz_clusters(X_2d,clus_names)



"""# Try AEs

## Try A Shallow Autoencoder
"""

import tensorflow as tf
import pandas as pd
from tensorflow.keras.layers import Dense, Input, BatchNormalization

df_nondvi=pickle.load(open('/content/drive/MyDrive/0Thiland Coordination/pickle_files/nondvi_withcity_lights.pkl','rb'))

X=df_nondvi.values
rows,cols=X.shape

latent_dim=5
ins=Input(shape=(cols,))
indec=Input(shape=(latent_dim,))
def encoder(inputs):
  x=Dense(100,activation='relu')(inputs)
  # x=Dense(100,activation='relu')(x)
  x=BatchNormalization()(x)
  x=Dense(int(cols//2),activation='relu')(x)
  x=BatchNormalization()(x)

  x=Dense(latent_dim,activation='relu')(x)
  
  enc_model=tf.keras.models.Model(inputs,x,name='ae_enc')
  return enc_model

def decoder(inputs):
  x=Dense(int(cols//2),activation='relu')(inputs)
  # x=Dense(200,activation='relu')(x)
  x=BatchNormalization()(x)

  x=Dense(100,activation='relu')(x)
  x=BatchNormalization()(x)

  x=Dense(cols,activation='tanh')(x)

  dec_model=tf.keras.models.Model(inputs,x,name='dec_model')
  return dec_model

enc_model=encoder(ins)
enc_out=enc_model(ins)
dec_model=decoder(indec)
dec_out=dec_model(enc_out)
ae_model=tf.keras.models.Model(ins,dec_out,name='ae')

ae_model.summary()

ae_model.compile(optimizer='adam',loss='mse',metrics=['mse'])

X_scaled=((df_nondvi-df_nondvi.mean())/df_nondvi.std()).values
X_scaled.mean(axis=0)

es_callback=tf.keras.callbacks.EarlyStopping(patience=10)
his=ae_model.fit(X_scaled,X_scaled,epochs=200,batch_size=8,validation_split=0.2,callbacks=[es_callback])

X_enc=enc_model(X_scaled)

tf.reduce_mean(X_enc,axis=0)

from sklearn.manifold import TSNE
ts=TSNE(2)
X_2d_ae=ts.fit_transform(X_enc_ae)

import matplotlib.pyplot as plt
plt.scatter(X_2d[:,0],X_2d[:,1])

from sklearn.cluster import AgglomerativeClustering

ag=AgglomerativeClustering(n_clusters=4)
ag.fit(X_enc)

evaluate_clustering(X_enc,ag.labels_)

np.unique(ag.labels_,return_counts=True)

idx_name={i:name for i,name in enumerate(df_nondvi.index)}
name_idx={name:i for i,name in enumerate(df_nondvi.index)}

# plot dendogram
# code from here :https://stackabuse.com/hierarchical-clustering-with-python-and-scikit-learn
from scipy.cluster.hierarchy import dendrogram, linkage

linked = linkage(X_enc,'ward')

labelList = range(len(X_enc))

plt.figure(figsize=(10, 12))
dendrogram(linked,
            orientation='top',
            labels=labelList,
            distance_sort='descending',
            show_leaf_counts=True)
plt.show()

ae_model.save('/content/drive/MyDrive/0Thiland Coordination/AE Models/ae_model_300_novi.h5')

enc_model.save('/content/drive/MyDrive/0Thiland Coordination/AE Models/enc_model_300_novi.h5')

ae_model=keras.models.load_model('/content/drive/MyDrive/0Thiland Coordination/AE Models/ae_model_300_novi.h5')
enc_model=keras.models.load_model('/content/drive/MyDrive/0Thiland Coordination/AE Models/enc_model_300_novi.h5')

X_enc_ae=enc_model(X_scaled)
ag_model,clus_names,clus,=agg_clustering(X_enc_ae,4)

clus_names

df_nondvi.loc[['New York','Philadelphia','Cairo','Paris'],:]

df_nondvi.loc[['New York','Vienna','Minsk','Damascus','Khartoum'],:]

ag,clus_names,k=agg_clustering(X_2d_ae,4)
# clus_names,k=cluster_with_kmeans(X_2d_ae,4)

viz_clusters(X_2d_ae,clus_names)

"""## Try VAE + Tanh"""

tf.compat.v1.enable_eager_execution()
tf.executing_eagerly()

df_all_cl=pickle.load(open('/content/drive/MyDrive/0Thiland Coordination/pickle_files/nondvi_withcity_lights.pkl','rb'))

import tensorflow as tf
keras=tf.keras
import tensorflow.keras.backend as K

latent_dim=5
X=df_all_cl.values
rows,cols=X.shape

class Sample(tf.keras.layers.Layer):
  def call(self,inputs):
    mu,sigma=inputs
    shapes=tf.shape(mu)
    batch=shapes[0]
    col=shapes[1]
    eps=K.random_normal((batch,col))
    Z=mu+eps*tf.exp(sigma*0.5)
    return Z

def encoder_layers(inputs,latent_dim):

  
  x=keras.layers.Dense(100,activation='relu')(inputs)
  x=keras.layers.BatchNormalization()(x)
  x=keras.layers.Dense(20,activation='relu')(x)
  x=keras.layers.BatchNormalization()(x)
  x=keras.layers.Dense(int(cols//2),activation='relu')(x)
  x=keras.layers.BatchNormalization()(x)
  mu=keras.layers.Dense(latent_dim)(x)
  sigma=keras.layers.Dense(latent_dim)(x)

  return mu,sigma

def encoder_model(input_shape,latent_dim):

  inputs=keras.layers.Input(shape=input_shape)
  mu,sigma=encoder_layers(inputs,latent_dim)

  Z=Sample()([mu,sigma])

  enc_model=keras.models.Model(inputs=inputs,outputs=[mu,sigma,Z])

  return enc_model

encoder=encoder_model(cols,latent_dim)

def decoder_layers(inputs):
  x=keras.layers.Dense(int(cols//2),activation='relu')(inputs)
  x=keras.layers.BatchNormalization()(x)

  x=keras.layers.Dense(20,activation='relu')(x)
  x=keras.layers.BatchNormalization()(x)
  x=keras.layers.Dense(100,activation='relu')(x)
  x=keras.layers.BatchNormalization()(x)

  x=keras.layers.Dense(cols,activation='tanh')(x)

  return x


def decoder_model(latent_dim):
  inputs=keras.layers.Input(shape=latent_dim)
  outpus=decoder_layers(inputs)

  dec_model=keras.models.Model(inputs,outpus)
  return dec_model

decoder=decoder_model(latent_dim)

decoder.summary()

def KL_loss(inputs, outputs, mu, sigma):

  los=tf.keras.losses.mean_squared_error(inputs,outputs)
  kl_loss = 1 + sigma - tf.square(mu) - tf.math.exp(sigma)
  return los+tf.reduce_mean(kl_loss) * -0.5

def mse_loss(inputs,outputs):

  los=tf.keras.losses.mean_squared_error(inputs,outputs)
  return los

def vae_model(input_shape,encoder,decoder):
  inputs=keras.layers.Input(shape=input_shape)
  mu,sigma,Z=encoder(inputs)
  dec_output=decoder(Z)
  model=keras.models.Model(inputs,dec_output)
  losskl=KL_loss(inputs,dec_output,mu,sigma)
  # mse = tf.keras.losses.mean_squared_error(inputs,dec_output)#mse_loss(inputs,dec_output)
  model.add_loss(losskl)
  # model.add_loss(mse)
  return model

vae=vae_model(cols,encoder,decoder)

X_scaled=(X-X.mean(axis=0))/X.std(axis=0)

vae.compile(optimizer='adam')

hist=vae.fit(X_scaled,X_scaled,epochs=200,validation_split=0.2)

vae=keras.models.load_model('/content/drive/MyDrive/0Thiland Coordination/AE Models/vae_model_linear_300_novi.h5',custom_objects={'Sample':Sample,'KL_loss':KL_loss})
encoder=keras.models.load_model('/content/drive/MyDrive/0Thiland Coordination/AE Models/encoder_tanh_300_novi.h5',custom_objects={'Sample':Sample,'KL_loss':KL_loss})

X_enc,_,_=encoder(X_scaled)



from sklearn.manifold import TSNE
ts=TSNE(2)
X_2d2=ts.fit_transform(X_enc)
import matplotlib.pyplot as plt
plt.scatter(X_2d2[:,0],X_2d2[:,1])

import matplotlib.pyplot as plt
plt.scatter(X_2d[:,0],X_2d[:,1])

from sklearn.cluster import AgglomerativeClustering,KMeans

ag=AgglomerativeClustering(n_clusters=3)
ag.fit(X_enc)

evaluate_clustering(X_enc2,ag.labels_)

np.unique(ag.labels_,return_counts=True)

idx_name={i:name for i,name in enumerate(df_all_cl.index)}
name_idx={name:i for i,name in enumerate(df_all_cl.index)}

# plot dendogram
# code from here :https://stackabuse.com/hierarchical-clustering-with-python-and-scikit-learn
from scipy.cluster.hierarchy import dendrogram, linkage

linked = linkage(X_enc,'ward')

labelList = range(len(X_enc))

plt.figure(figsize=(10, 12))
dendrogram(linked,
            orientation='top',
            labels=labelList,
            distance_sort='descending',
            show_leaf_counts=True)
plt.show()

vae.save('/content/drive/MyDrive/0Thiland Coordination/AE Models/vae_model_tanh_300_novi.h5')

encoder.save('/content/drive/MyDrive/0Thiland Coordination/AE Models/encoder_tanh_300_novi.h5')

ag_model,clus_names,clus,=agg_clustering(X_enc,3)

viz_clusters(X_2d2,clus_names)

#kmeans
clus_names,clus,=cluster_with_kmeans(X_2d2,4)

viz_clusters(X_2d2,clus_names)





"""## Try VAE + Linear"""

tf.compat.v1.enable_eager_execution()
tf.executing_eagerly()

df_nondvi=pd.read_csv('/content/drive/MyDrive/0Thiland Coordination/CSV Datasets/Soc_econ_data_4paper.csv')
df_nondvi.set_index('eFUA_name',inplace=True)
X=df_nondvi.values
rows,cols=X.shape

X_scaled=((df_nondvi-df_nondvi.mean())/df_nondvi.std()).values
X_scaled.mean(axis=0)

import tensorflow as tf
keras=tf.keras
import tensorflow.keras.backend as K

latent_dim=9

class Sample(tf.keras.layers.Layer):
  def call(self,inputs):
    mu,sigma=inputs
    shapes=tf.shape(mu)
    batch=shapes[0]
    col=shapes[1]
    eps=K.random_normal((batch,col))
    Z=mu+eps*tf.exp(sigma*0.5)
    return Z

def encoder_layers(inputs,latent_dim):

  ## Encoder Layers

  x=keras.layers.Dense(int(16),activation='relu')(inputs)
  x=keras.layers.BatchNormalization()(x)
  x=keras.layers.Dense(int(8),activation='relu')(x)
  x=keras.layers.BatchNormalization()(x)
  mu=keras.layers.Dense(latent_dim)(x)
  sigma=keras.layers.Dense(latent_dim)(x)

  return mu,sigma

def encoder_model(input_shape,latent_dim):

  inputs=keras.layers.Input(shape=input_shape)
  mu,sigma=encoder_layers(inputs,latent_dim)

  Z=Sample()([mu,sigma])

  enc_model=keras.models.Model(inputs=inputs,outputs=[mu,sigma,Z])

  return enc_model



def decoder_layers(inputs):

  ## Decoder Layers
  x=keras.layers.Dense(6,activation='relu')(inputs)
  x=keras.layers.BatchNormalization()(x)
  x=keras.layers.Dense(cols,activation='relu')(x)
  x=keras.layers.BatchNormalization()(x)
  x=keras.layers.Dense(cols,activation='linear')(x)

  return x


def decoder_model(latent_dim):
  inputs=keras.layers.Input(shape=latent_dim)
  outpus=decoder_layers(inputs)

  dec_model=keras.models.Model(inputs,outpus)
  return dec_model

def KL_loss(inputs, outputs, mu, sigma):

  los=tf.keras.losses.mean_squared_error(inputs,outputs)
  kl_loss = -0.5 * (1 + sigma - tf.square(mu) - tf.exp(sigma))
  kl_loss = tf.reduce_mean(tf.reduce_sum(kl_loss, axis=1))
  # kl_loss = (1 + sigma - tf.square(mu) - tf.math.exp(sigma))* -0.5
  return kl_loss+los

def mse_loss(inputs,outputs):

  los=tf.keras.losses.mean_squared_error(inputs,outputs)
  return los

def vae_model(input_shape,encoder,decoder):
  inputs=keras.layers.Input(shape=input_shape)
  mu,sigma,Z=encoder(inputs)
  dec_output=decoder(Z)
  model=keras.models.Model(inputs,dec_output)
  losskl=KL_loss(inputs,dec_output,mu,sigma)
  # mse = tf.keras.losses.mean_squared_error(inputs,dec_output)#mse_loss(inputs,dec_output)
  model.add_loss(losskl)
  # model.add_loss(mse)
  return model

encoder=encoder_model(cols,latent_dim)
decoder=decoder_model(latent_dim)
vae=vae_model(cols,encoder,decoder)

vae.compile(optimizer='adam',metrics=['mse'])

hist=vae.fit(X_scaled,X_scaled,epochs=30)

# try loaidng the model:
# vae=keras.models.load_model('/content/drive/MyDrive/0Thiland Coordination/AE Models/vae_model_linear_300_novi.h5',custom_objects=Sample)

X_enc,_,_=encoder(X_scaled)

np.std(X_enc,axis=0)



from sklearn.manifold import TSNE
from sklearn.decomposition import PCA

ts=TSNE(2)
X_2d=ts.fit_transform(X_enc)

import matplotlib.pyplot as plt
plt.scatter(X_2d[:,0],X_2d[:,1])

from sklearn.cluster import AgglomerativeClustering,KMeans

cls_scr_sc={}
cls_scr_r={}
for n_c in range(3,10):
  ag=AgglomerativeClustering(n_clusters=n_c,linkage='ward')
  ag.fit(X_enc)
  r=evaluate_clustering(X_enc,ag.labels_)
  print(r)
  x,y=np.unique(ag.labels_,return_counts=True)
  print(y)
  cls_scr_sc[n_c]=r[-1]
  cls_scr_r[n_c]=r[0]

cls_scr_sc={}
cls_scr_r={}
for n_c in range(3,10):
  ag=AgglomerativeClustering(n_clusters=n_c,linkage='ward')
  ag.fit(X_enc)
  r=evaluate_clustering(X_enc,ag.labels_)
  print(r)
  x,y=np.unique(ag.labels_,return_counts=True)
  print(y)
  cls_scr_sc[n_c]=r[-1]
  cls_scr_r[n_c]=r[0]

evaluate_clustering(X_2d,ag.labels_)

plt.bar([0,1,2,3,4,5],[  47, 1164, 2254, 1828, 1509, 1792])

idx_name={i:name for i,name in enumerate(df_nondvi.index)}
name_idx={name:i for i,name in enumerate(df_nondvi.index)}

df_nondvi.describe().iloc[1:,].to_csv('soc_econ_data_stats.csv')

# plot dendogram
# code from here :https://stackabuse.com/hierarchical-clustering-with-python-and-scikit-learn
from scipy.cluster.hierarchy import dendrogram, linkage

linked = linkage(X_enc,'ward')

labelList = range(len(X_enc))

plt.figure(figsize=(10, 12))
dendrogram(linked,
            orientation='top',
            labels=labelList,
            distance_sort='descending',
            show_leaf_counts=True)
plt.show()

vae.save('/content/drive/MyDrive/0Thiland Coordination/AE Models/vae_model_linear_soc-econ-v2.h5')



encoder.save('/content/drive/MyDrive/0Thiland Coordination/AE Models/vae_model_linear_soc-econ-v2.h5')

ag_model,clus_names,clus,=agg_clustering(X_enc,6)

viz_clusters(X_2d,clus_names)



viz_clusters(X_2d,clus_names)

#kmeans
clus_names,clus,=cluster_with_kmeans(X_enc,2)

viz_clusters(X_2d,clus_names)



"""## Try VAE High latent dim (up to soil temp)"""

# tf.compat.v1.enable_eager_execution()
# tf.executing_eagerly()

df_all_cl=pickle.load(open('/content/drive/MyDrive/0Thiland Coordination/all_data_with_st_8k.pickle','rb'))

import tensorflow as tf
keras=tf.keras
import tensorflow.keras.backend as K
from tensorflow.keras import regularizers
from sklearn.model_selection import train_test_split

df_all_cl.dropna(inplace=True)
X=df_all_cl.values
X_scaled=(X-X.mean(axis=0))/X.std(axis=0)
rows,cols=X.shape
latent_dim=int(cols*0.75)

X_scaled.shape

X_tr,X_val=train_test_split(X_scaled,test_size=0.2)

class Sample(tf.keras.layers.Layer):
  def call(self,inputs):
    mu,sigma=inputs
    shapes=tf.shape(mu)
    batch=shapes[0]
    col=shapes[1]
    eps=K.random_normal((batch,col))
    Z=mu+eps*tf.exp(sigma*0.5)
    return Z



def encoder_layers(inputs,latent_dim):

  
  x=keras.layers.Dense(128,activation='relu')(inputs)
  x=keras.layers.BatchNormalization()(x)
  # x=keras.layers.Dropout(0.2)(x)

  x=keras.layers.Dense(128,activation='relu')(x)
  x=keras.layers.BatchNormalization()(x)
  # x=keras.layers.Dropout(0.2)(x)

  x=keras.layers.Dense(64,activation='relu')(x)
  x=keras.layers.BatchNormalization()(x)
  # x=keras.layers.Dropout(0.2)(x)

  x=keras.layers.Dense(int(64),activation='relu')(x)
  x=keras.layers.BatchNormalization()(x)
  # x=keras.layers.Dropout(0.2)(x)

  mu=keras.layers.Dense(latent_dim)(x)
  sigma=keras.layers.Dense(latent_dim)(x)

  return mu,sigma

def encoder_model(input_shape,latent_dim):

  inputs=keras.layers.Input(shape=input_shape)
  mu,sigma=encoder_layers(inputs,latent_dim)

  Z=Sample()([mu,sigma])

  enc_model=keras.models.Model(inputs=inputs,outputs=[mu,sigma,Z])

  return enc_model

def decoder_layers(inputs):

  x=keras.layers.Dense(int(32),activation='relu')(inputs)
  x=keras.layers.BatchNormalization()(x)
  # x=keras.layers.Dropout(0.2)(x)

  x=keras.layers.Dense(32,activation='relu')(x)
  x=keras.layers.BatchNormalization()(x)
  # x=keras.layers.Dropout(0.2)(x)

  x=keras.layers.Dense(40,activation='relu')(x)
  x=keras.layers.BatchNormalization()(x)
  # x=keras.layers.Dropout(0.2)(x)

  x=keras.layers.Dense(cols,activation='linear')(x)

  return x


def decoder_model(latent_dim):
  inputs=keras.layers.Input(shape=latent_dim)
  outpus=decoder_layers(inputs)

  dec_model=keras.models.Model(inputs,outpus)
  return dec_model





def KL_loss(inputs, outputs, mu, sigma):

  los=tf.keras.losses.mean_squared_error(inputs,outputs)
  kl_loss = -0.5 * (1 + sigma - tf.square(mu) - tf.exp(sigma))
  kl_loss = tf.reduce_mean(tf.reduce_sum(kl_loss, axis=1))
  # kl_loss = (1 + sigma - tf.square(mu) - tf.math.exp(sigma))* -0.5
  return kl_loss+los

def mse_loss(inputs,outputs):

  los=tf.keras.losses.mean_squared_error(inputs,outputs)
  return los

def add_l2_regularization(layer):
    def _add_l2_regularization():
        l2 = tf.keras.regularizers.l2(1e-4)
        return l2(layer.kernel)
    return _add_l2_regularization

def vae_model(input_shape,encoder,decoder):
  inputs=keras.layers.Input(shape=input_shape)
  mu,sigma,Z=encoder(inputs)
  dec_output=decoder(Z)
  model=keras.models.Model(inputs,dec_output)
  losskl=KL_loss(inputs,dec_output,mu,sigma)
  # mse = tf.keras.losses.mean_squared_error(inputs,dec_output)#mse_loss(inputs,dec_output)
  model.add_loss(losskl)
  # for i in range(len(encoder.layers)):
  #   layer = encoder.layers[i]
  #   if isinstance(layer, (tf.keras.layers.Dense)):
  #       print("added 1")
  #       model.add_loss(add_l2_regularization(layer))
  
  # for i in range(len(decoder.layers)):
  #   layer = decoder.layers[i]
  #   if isinstance(layer, (tf.keras.layers.Dense)):
  #       print("added 1")
  #       model.add_loss(add_l2_regularization(layer))
  # model.add_loss(mse)
  return model

encoder=encoder_model(cols,latent_dim)
decoder=decoder_model(latent_dim)
vae=vae_model(cols,encoder,decoder)
vae.summary()

keras.utils.plot_model(decoder)

vae.compile(optimizer='nadam',metrics=['mse'])



hist=vae.fit(X_scaled,X_scaled,epochs=100,validation_split=0.1)

X_enc,_,_=encoder(X_scaled)
X_dec=vae(X_scaled)

tf.reduce_mean(X_enc,axis=0),tf.math.reduce_std(X_enc,axis=0)

s=tf.reduce_sum(tf.square(X_enc),axis=0)

plt.scatter(list(range(len(s))),s)

from sklearn.manifold import TSNE
ts=TSNE(2)
X_2d2=ts.fit_transform(X_enc)
import matplotlib.pyplot as plt
plt.scatter(X_2d2[:,0],X_2d2[:,1])



from sklearn.manifold import TSNE
ts=TSNE(2)
X_2d2=ts.fit_transform(X_enc)
import matplotlib.pyplot as plt
plt.scatter(X_2d2[:,0],X_2d2[:,1])

from sklearn.manifold import TSNE
ts=TSNE(2)
X_2d2=ts.fit_transform(X_enc)
import matplotlib.pyplot as plt
plt.scatter(X_2d2[:,0],X_2d2[:,1])

# keras.models.save_model(vae,'/content/drive/MyDrive/0Thiland Coordination/AE Models/vae_with_st_8k_v2.h5',)#custom_objects={'Sample':Sample,'KL_loss':KL_loss})
# keras.models.save_model(encoder,'/content/drive/MyDrive/0Thiland Coordination/AE Models/encoder_with_st_8k_v2.h5',)#custom_objects={'Sample':Sample,'KL_loss':KL_loss})

from sklearn.manifold import TSNE
ts=TSNE(2)
X_2d2=ts.fit_transform(X_enc)
import matplotlib.pyplot as plt
plt.scatter(X_2d2[:,0],X_2d2[:,1])

from sklearn.manifold import TSNE
ts=TSNE(2)
X_2d2=ts.fit_transform(X_enc)
import matplotlib.pyplot as plt
plt.scatter(X_2d2[:,0],X_2d2[:,1])

# keras.models.save_model(vae,'/content/drive/MyDrive/0Thiland Coordination/AE Models/vae_biglatent_linear_8k_novi.h5',)#custom_objects={'Sample':Sample,'KL_loss':KL_loss})
# keras.models.save_model(encoder,'/content/drive/MyDrive/0Thiland Coordination/AE Models/encoder_biglatent_8k_novi.h5',)#custom_objects={'Sample':Sample,'KL_loss':KL_loss})

from sklearn.cluster import AgglomerativeClustering,KMeans,AffinityPropagation

ag=AgglomerativeClustering(n_clusters=8)
ag.fit(X_2d2)

evaluate_clustering(X_2d2,ag.labels_)

x,y=np.unique(ag.labels_,return_counts=True)

#plot dis
plt.bar(x,y)

idx_name={i:name for i,name in enumerate(df_all_cl.index)}
name_idx={name:i for i,name in enumerate(df_all_cl.index)}

# plot dendogram
# code from here :https://stackabuse.com/hierarchical-clustering-with-python-and-scikit-learn
from scipy.cluster.hierarchy import dendrogram, linkage

linked = linkage(X_2d2,'ward')

labelList = range(len(X_2d2))

plt.figure(figsize=(10, 12))
dendrogram(linked,
            orientation='top',
            labels=labelList,
            distance_sort='descending',
            show_leaf_counts=True)
plt.show()

ag_model,clus_names,clus,=agg_clustering(X_2d2,8,link='single')

viz_clusters(X_2d2,clus_names)

viz_clusters(X_2d2,clus_names)

#kmeans
clus_names,clus,=cluster_with_kmeans(X_2d2,8)

viz_clusters(X_2d2,clus_names)

def get_cluster_of(name,clus_dict):
  for k in clus_dict:
    if name in clus_dict[k]:
      return k

def sample_from_clusters(df,clust_dict,n_samples=2):
  samples=[]
  for k in clust_dict:
    for j in range(n_samples):
      samples.append(np.random.choice(clust_dict[k]))
  
  return df.loc[samples,:],samples

get_cluster_of('New York',clus_names),get_cluster_of('Cairo',clus_names) ,get_cluster_of('Paris',clus_names) ,get_cluster_of('Minsk',clus_names),get_cluster_of('Vienna',clus_names),get_cluster_of('Khartoum',clus_names)

sdf,n=sample_from_clusters(df_all_cl,clus_names)
sdf

"""## Try multi input High latent dim"""

# tf.compat.v1.enable_eager_execution()
# tf.executing_eagerly()

df_all_cl=pickle.load(open('/content/drive/MyDrive/0Thiland Coordination/all_data_with_st_8k.pickle','rb'))



df_all_cl.columns

plt.scatter(df_all_cl.iloc[:,[0]],df_all_cl.iloc[:,[4]])
plt.show()
plt.scatter(df_all_cl.loc[:,['1990_mean']],df_all_cl.loc[:,['CL2015']])
plt.show()
plt.scatter(df_all_cl.loc[:,['2015_mean']],df_all_cl.loc[:,['2015_Pop']])

df_all_cl.corr()



# split data
area_cols=['1990_area', '2000_area', '2015_area', 'water']
pop_cols=['1990_Pop', '2000_Pop','2015_Pop']
cl_cols=['CL2014', 'CL2013', 'CL2018', 'CL2016', 'CL2019', 'CL2020',
       'CL2015', 'CL2017', 'CL2012']
vi_cols=['1990_mean', '1990_median', '1990_std',
       '2000_mean', '2000_median', '2000_std', '2015_mean', '2015_median',
       '2015_std', '1990_range', '2000_range', '2015_range',]

st_cols=['stl1_1990_average', 'stl1_1990_median', 'stl1_1990_std', 'stl1_1990_range',
       'stl1_2000_average', 'stl1_2000_median', 'stl1_2000_std', 'stl1_2000_range',
       'stl1_2015_average', 'stl1_2015_median', 'stl1_2015_std', 'stl1_2015_range']

area_df,pop_df,ndvi_df,cl_df,st_df=df_all_cl.loc[:,area_cols],df_all_cl.loc[:,pop_cols],df_all_cl.loc[:,vi_cols],df_all_cl.loc[:,cl_cols],df_all_cl.loc[:,st_cols]

cl_df.sort_index(axis=1,inplace=True)



# add pct change to area,pop,st
suf="_change"
area_df_pct=area_df.pct_change(axis='columns').drop(['1990_area','water'],axis=1)
pop_df_pct=pop_df.pct_change(axis='columns').drop([pop_df.columns[0],],axis=1)
cl_df_pct=cl_df.pct_change(axis='columns').drop([cl_df.columns[0],],axis=1)

pct_dfs={'cl_df_pct':cl_df_pct,'pop_df_pct':pop_df_pct,'area_df_pct':area_df_pct}

for df in pct_dfs.values():
  df.replace({np.nan:0,np.inf:0},inplace=True)
  
  new_cols=[]
  for col in df.columns:
    col+=suf
    new_cols.append(col)
  df.columns=new_cols

pop_df=pd.concat([pop_df,pop_df_pct],axis=1)
area_df=pd.concat([area_df,area_df_pct],axis=1)
cl_df=pd.concat([cl_df,cl_df_pct],axis=1)

cl_df



df_all_cl2=pd.concat([area_df,pop_df,ndvi_df,cl_df,st_df],axis=1)

df_all_cl2.dropna(inplace=True)

area_df.dropna(inplace=True)

pop_df.dropna(inplace=True)
st_df.dropna(inplace=True)
cl_df.dropna(inplace=True)



ndvi_df=ndvi_df.loc[st_df.index,:]
cl_df=cl_df.loc[st_df.index,:]
pop_df=pop_df.loc[st_df.index,:]
area_df=area_df.loc[st_df.index,:]

area_df=(area_df-area_df.mean())/area_df.std()
pop_df=(pop_df-pop_df.mean())/pop_df.std()
ndvi_df=(ndvi_df-ndvi_df.mean())/ndvi_df.std()
cl_df=(cl_df-cl_df.mean())/cl_df.std()
st_df=(st_df-st_df.mean())/st_df.std()

area_df.std()

import tensorflow as tf
keras=tf.keras
import tensorflow.keras.backend as K
from tensorflow.keras import regularizers
from sklearn.model_selection import train_test_split

X=df_all_cl2.values
X_scaled=(X-X.mean(axis=0))/X.std(axis=0)
rows,cols=X.shape
latent_dim=int(cols*.5)

cl_df.isna().sum(),area_df.isna().sum(),pop_df.isna().sum()

X_tr,X_val=train_test_split(X_scaled,test_size=0.2)



"""### VAE Arch"""

class Sample(tf.keras.layers.Layer):
  def call(self,inputs):
    mu,sigma=inputs
    shapes=tf.shape(mu)
    batch=shapes[0]
    col=shapes[1]
    eps=K.random_normal((batch,col))
    Z=mu+eps*tf.exp(sigma*0.5)
    return Z



def encoder_layers(inputs,shapes,latent_dim):

  a_x,p_x,n_x,c_x,s_x=inputs
  area_shape,pop_shape,ndvi_shape,cl_shape,st_shape=shapes


  #2 dense layers each
  a_x=keras.layers.Dense(int(area_shape*2),activation='relu')(a_x)
  a_x=keras.layers.BatchNormalization()(a_x)
  a_x=keras.layers.Dense(int(area_shape),activation='relu')(a_x)
  a_x=keras.layers.BatchNormalization()(a_x)

  p_x=keras.layers.Dense(int(pop_shape*2),activation='relu')(p_x)
  p_x=keras.layers.BatchNormalization()(p_x)
  p_x=keras.layers.Dense(int(pop_shape),activation='relu')(p_x)
  p_x=keras.layers.BatchNormalization()(p_x)

  n_x=keras.layers.Dense(int(ndvi_shape*2),activation='relu')(n_x)
  n_x=keras.layers.BatchNormalization()(n_x)

  n_x=keras.layers.Dense(int(ndvi_shape),activation='relu')(n_x)
  n_x=keras.layers.BatchNormalization()(n_x)


  c_x=keras.layers.Dense(int(cl_shape*2),activation='relu')(c_x)
  c_x=keras.layers.BatchNormalization()(c_x)

  c_x=keras.layers.Dense(int(cl_shape),activation='relu')(c_x)
  c_x=keras.layers.BatchNormalization()(c_x)

  s_x=keras.layers.Dense(int(st_shape*2),activation='relu')(s_x)
  s_x=keras.layers.BatchNormalization()(s_x)

  s_x=keras.layers.Dense(int(st_shape),activation='relu')(s_x)
  s_x=keras.layers.BatchNormalization()(s_x)


  x=keras.layers.Concatenate()([a_x,p_x,n_x,c_x,s_x])

  x=keras.layers.Dense(64,activation='relu')(x)
  x=keras.layers.BatchNormalization()(x)
  # x=keras.layers.Dropout(0.2)(x)


  # x=keras.layers.Dense(32,activation='relu')(x)
  # x=keras.layers.BatchNormalization()(x)
  # # x=keras.layers.Dropout(0.2)(x)

  x=keras.layers.Dense(int(64),activation='relu')(x)
  x=keras.layers.BatchNormalization()(x)
  # x=keras.layers.Dropout(0.2)(x)

  mu=keras.layers.Dense(latent_dim)(x)
  sigma=keras.layers.Dense(latent_dim)(x)

  return mu,sigma

def encoder_model(inputs,latent_dim):
  area,pop,ndvi,cl,st=inputs
  
  area_shape=(area.shape)
  area_shape=area_shape[1]

  pop_shape=(pop.shape)
  pop_shape=pop_shape[1]

  ndvi_shape=(ndvi.shape)
  ndvi_shape=ndvi_shape[1]

  cl_shape=(cl.shape)
  cl_shape=cl_shape[1]

  st_shape=(st.shape)
  st_shape=st_shape[1]
  
  a_x=keras.layers.Input(shape=area_shape)
  p_x=keras.layers.Input(shape=pop_shape)
  n_x=keras.layers.Input(shape=ndvi_shape)
  c_x=keras.layers.Input(shape=cl_shape)
  s_x=keras.layers.Input(shape=st_shape)

  shapes=[area_shape,pop_shape,ndvi_shape,cl_shape,st_shape]
  mu,sigma=encoder_layers([a_x,p_x,n_x,c_x,s_x],shapes,latent_dim)

  Z=Sample()([mu,sigma])

  enc_model=keras.models.Model(inputs=[a_x,p_x,n_x,c_x,s_x],outputs=[mu,sigma,Z])

  return enc_model,shapes

def decoder_layers(inputs):

  x=keras.layers.Dense(int(32),activation='relu')(inputs)
  x=keras.layers.BatchNormalization()(x)
  # x=keras.layers.Dropout(0.4)(x)

  x=keras.layers.Dense(32,activation='relu')(x)
  x=keras.layers.BatchNormalization()(x)
  # # x=keras.layers.Dropout(0.2)(x)

  x=keras.layers.Dense(40,activation='relu')(x)
  x=keras.layers.BatchNormalization()(x)
  
  x=keras.layers.Dense(40,activation='relu')(x)
  x=keras.layers.BatchNormalization()(x)
  # x=keras.layers.Dropout(0.4)(x)

  x=keras.layers.Dense(cols,activation='linear')(x)

  return x


def decoder_model(latent_dim):
  inputs=keras.layers.Input(shape=latent_dim)
  outpus=decoder_layers(inputs)

  dec_model=keras.models.Model(inputs,outpus)
  return dec_model

def KL_loss(inputs, outputs, mu, sigma):

  con_inputs=keras.layers.Concatenate()(inputs)
  los=tf.keras.losses.mean_squared_error(con_inputs,outputs)
  kl_loss = -0.5 * (1 + sigma - tf.square(mu) - tf.exp(sigma))
  kl_loss = tf.reduce_mean(tf.reduce_sum(kl_loss, axis=1))
  # kl_loss = (1 + sigma - tf.square(mu) - tf.math.exp(sigma))* -0.5
  return kl_loss+los

def mse_loss(inputs,outputs):

  los=tf.keras.losses.mean_squared_error(inputs,outputs)
  return los

def add_l2_regularization(layer):
    def _add_l2_regularization():
        l2 = tf.keras.regularizers.l2(1e-4)
        return l2(layer.kernel)
    return _add_l2_regularization

def vae_model(input_shapes,encoder,decoder):
  area_shape,pop_shape,ndvi_shape,cl_shape,st_shape=shapes

  a_x=keras.layers.Input(shape=area_shape)
  p_x=keras.layers.Input(shape=pop_shape)
  n_x=keras.layers.Input(shape=ndvi_shape)
  c_x=keras.layers.Input(shape=cl_shape)
  s_x=keras.layers.Input(shape=st_shape)

  inputs=[a_x,p_x,n_x,c_x,s_x]
  mu,sigma,Z=encoder(inputs)
  dec_output=decoder(Z)

  model=keras.models.Model(inputs,dec_output)
  losskl=KL_loss(inputs,dec_output,mu,sigma)
  # mse = tf.keras.losses.mean_squared_error(inputs,dec_output)#mse_loss(inputs,dec_output)
  model.add_loss(losskl)
  # for i in range(len(encoder.layers)):
  #   layer = encoder.layers[i]
  #   if isinstance(layer, (tf.keras.layers.Dense)):
  #       print("added 1")
  #       model.add_loss(add_l2_regularization(layer))
  
  # for i in range(len(decoder.layers)):
  #   layer = decoder.layers[i]
  #   if isinstance(layer, (tf.keras.layers.Dense)):
  #       print("added 1")
  #       model.add_loss(add_l2_regularization(layer))
  # model.add_loss(mse)
  return model

"""### VAE Training"""

org_inputs=([area_df.values,pop_df.values,ndvi_df.values,cl_df.values,st_df.values])
inputs=org_inputs.copy()
mask=np.array([1,1,1,1,1])

for i,inp in enumerate(inputs):
  inp*=mask[i]

org_inputs

encoder,shapes=encoder_model(inputs,latent_dim)
decoder=decoder_model(latent_dim)

vae=vae_model(shapes,encoder,decoder)

encoder.summary()

vae.compile(optimizer='adam',metrics=['mse'])

outputs=np.concatenate(inputs,axis=1)

hist=vae.fit(inputs,outputs,epochs=15)

X_enc,_,_=encoder(inputs)
# X_dec=vae(X_scaled)

tf.reduce_mean(X_enc,axis=0),tf.math.reduce_std(X_enc,axis=0)

s=tf.reduce_sum(tf.square(X_enc),axis=0)

plt.scatter(list(range(len(s))),s)

plt.scatter(list(range(len(s))),s)

encoder2=keras.models.load_model('/content/drive/MyDrive/0Thiland Coordination/AE Models/encoder_multiinput_slow_tr2.h5',custom_objects={'Sample':Sample,'KL_loss':KL_loss})

X_enc2,_,_=encoder2(inputs)

from sklearn.manifold import TSNE
from sklearn.decomposition import PCA
ts=TSNE(2)
X_2d2=ts.fit_transform(X_enc2)
import matplotlib.pyplot as plt
plt.scatter(X_2d2[:,0],X_2d2[:,1])

from sklearn.manifold import TSNE
ts=TSNE(2)
X_2d2=ts.fit_transform(X_enc)
import matplotlib.pyplot as plt
plt.scatter(X_2d2[:,0],X_2d2[:,1])

from sklearn.manifold import TSNE
ts=TSNE(2)
X_2d2=ts.fit_transform(X_enc)
import matplotlib.pyplot as plt
plt.scatter(X_2d2[:,0],X_2d2[:,1])

from sklearn.manifold import TSNE
ts=TSNE(2)
X_2d2=ts.fit_transform(X_enc)
import matplotlib.pyplot as plt
plt.scatter(X_2d2[:,0],X_2d2[:,1])

from sklearn.manifold import TSNE
ts=TSNE(2)
X_2d2=ts.fit_transform(X_enc)
import matplotlib.pyplot as plt
plt.scatter(X_2d2[:,0],X_2d2[:,1])





from sklearn.manifold import TSNE
ts=TSNE(2)
X_2d2=ts.fit_transform(X_enc)
import matplotlib.pyplot as plt
plt.scatter(X_2d2[:,0],X_2d2[:,1])

from sklearn.manifold import TSNE
ts=TSNE(2)
X_2d2=ts.fit_transform(X_enc)
import matplotlib.pyplot as plt
plt.scatter(X_2d2[:,0],X_2d2[:,1])

keras.models.save_model(vae,'/content/drive/MyDrive/0Thiland Coordination/AE Models/vae_multiinput_slow_tr4.h5',)#custom_objects={'Sample':Sample,'KL_loss':KL_loss})
keras.models.save_model(encoder,'/content/drive/MyDrive/0Thiland Coordination/AE Models/encoder_multiinput_slow_tr4.h5',)#custom_objects={'Sample':Sample,'KL_loss':KL_loss})



"""## Visulization, Evalution and Expoloration"""

from sklearn.cluster import AgglomerativeClustering,KMeans,AffinityPropagation

for n_c in range(6,20):
  ag=AgglomerativeClustering(n_clusters=n_c,linkage='single')
  ag.fit(X_enc2)
  print(evaluate_clustering(X_enc2,ag.labels_))
  x,y=np.unique(ag.labels_,return_counts=True)
  print(y)





plt.bar(x,y)

#plot dis
plt.bar(x,y)

idx_name={i:name for i,name in enumerate(df_all_cl2.index)}
name_idx={name:i for i,name in enumerate(df_all_cl2.index)}

# plot dendogram
# code from here :https://stackabuse.com/hierarchical-clustering-with-python-and-scikit-learn
from scipy.cluster.hierarchy import dendrogram, linkage

linked = linkage(X_enc2,'ward')

labelList = range(len(X_2d2))

plt.figure(figsize=(10, 12))
dendrogram(linked,
            orientation='top',
            labels=labelList,
            distance_sort='descending',
            show_leaf_counts=True)
plt.show()

ag_model,clus_names,clus,=agg_clustering(X_2d2,8)

viz_clusters(X_2d2,clus_names)

viz_clusters(X_2d2,clus_names)

#kmeans
clus_names,clus,=cluster_with_kmeans(X_2d2,8)

viz_clusters(X_2d2,clus_names)



"""### Exploration

"""

def get_cluster_of(name,clus_dict):
  for k in clus_dict:
    if name in clus_dict[k]:
      return k



def get_distance(pt1,pt2):
  return np.linalg.norm(pt1-pt2,axis=1)

df_all_cl2.loc[['New York'],]-1

get_distance(X_enc2[mask],X_enc2[7418])[[7415,7416]]

get_distance(X_scaled[mask],X_scaled[7418])[[7415,7416]]

mask=np.ones(len(X_scaled),bool)
mask[7418]=False

clus_names[1]

get_cluster_of('New York',clus_names),get_cluster_of('Cairo',clus_names) ,get_cluster_of('Paris',clus_names) ,get_cluster_of('Minsk',clus_names),get_cluster_of('Vienna',clus_names),get_cluster_of('Khartoum',clus_names)

df_all_cl2.loc[['Cairo','Shanghai','Seoul']]

df_all_cl2.loc[['Cairo','Minsk','New York'],]

def get_cluster_centers(df,clus_names):
  clus_means={}
  closest,furth={},{ }
  for k in clus_names:
    cluster_idx=clus_names[k]
    _clus_data=df.loc[cluster_idx]
    _mean=_clus_data.mean()
    dist=np.linalg.norm(_mean - _clus_data,axis=1)
    min_pt,max_pt=np.argmin(dist),np.argmax(dist)
    _min_c,max_c=_clus_data.loc[[cluster_idx[min_pt],]],_clus_data.loc[[cluster_idx[max_pt],]]
    closest[k]=_min_c.index.values[0]
    furth[k]=max_c.index.values[0]
    clus_means[k]=pd.DataFrame(_mean).transpose()
    
  
  return clus_means,closest,furth

ms,cl,f=get_cluster_centers(df_all_cl2,clus_names)

cluster_ana_dict={'means':ms,'closest':cl,'furthest':f}
pickle.dump(cluster_ana_dict,open('/content/drive/MyDrive/0Thiland Coordination/cluster_ana_dict.pkl','wb'))

df_all_cl.loc[['Thul','Hyderabad']]

df_all_cl2.sort_values('2015_area_change',ascending=False)

def plt_ranges_of_clusters(df,clus_names):
  pop_ls=['1990_Pop','2015_Pop']
  area_ls=['1990_area','2015_area']
  cl_ls=['CL2015','CL2020']
  st_ls=['stl1_1990_average','stl1_2015_average']
  ni_ls=['1990_mean','2015_mean']
  ni_med=['1990_median','2015_median']
  st_med=['stl1_1990_median','stl1_2015_median']
  area_ch=['2000_area_change','2015_area_change']


  plt_names={'population':pop_ls,'area':area_ls,'Area Change':area_ch,'citylight':cl_ls,'soil temp mean':st_ls,'soil temp median': st_med,'veg index mean':ni_ls,
             'veg index median': ni_med}

  for name in plt_names:
    for k in clus_names:
      _idx=clus_names[k]
      dt=df.loc[_idx,plt_names[name]].values
      plt.scatter(dt[:,0],dt[:,1])
    plt.title(name)
    plt.show()

plt_ranges_of_clusters(df_all_cl2,clus_names)

import json
with open("/content/drive/MyDrive/0Thiland Coordination/clusters_names_st.json", "w",encoding='utf8') as outfile: 
    json.dump({int(k):clus_names[k] for k in clus_names }, outfile,ensure_ascii=False)

# compare to direct input
ag_model,clus_names_dir,clus,=agg_clustering(X_scaled,8)

evaluate_clustering(X_scaled,ag_model.labels_)

x,y=np.unique(ag_model.labels_,return_counts=True)
plt.bar(x,y)

plt_ranges_of_clusters(df_all_cl2,clus_names_dir)

get_cluster_of('New York',clus_names_dir),get_cluster_of('Shanghai',clus_names_dir) ,get_cluster_of('Paris',clus_names_dir) ,get_cluster_of('Minsk',clus_names_dir),get_cluster_of('Vienna',clus_names_dir),get_cluster_of('Khartoum',clus_names_dir)

for k in clus_names_dir: print(k,len(clus_names_dir[k]))

df_all_cl2.loc[:,['1990_Pop','2015_Pop']].sort_values('1990_Pop').iloc[0,].values

def get_min_max_df(df,cols):
  mins=[]
  maxs=[]
  for col in cols:
    mins.append(df.loc[:,cols].sort_values(col).iloc[0,].values)
    maxs.append(df.loc[:,cols].sort_values(col,ascending=False).iloc[0,].values)
  
  return mins,maxs

def plt_ranges_single_cluster(df,clus_names):
  pop_ls=['1990_Pop','2015_Pop']
  area_ls=['1990_area','2015_area']
  cl_ls=['CL2015','CL2020']
  st_ls=['stl1_1990_average','stl1_2015_average']
  ni_ls=['1990_mean','2015_mean']
  ni_med=['1990_median','2015_median']
  st_med=['stl1_1990_median','stl1_2015_median']
  area_ch=['2000_area_change','2015_area_change']


  plt_names={'population':pop_ls,'area':area_ls,'Area Change':area_ch,'citylight':cl_ls,'soil temp mean':st_ls,'soil temp median': st_med,'veg index mean':ni_ls,
             'veg index median': ni_med}


  for name in plt_names:
    for k in clus_names:
      _idx=clus_names[k]
      dt=df.loc[_idx,plt_names[name]].values
      plt.scatter(dt[:,0],dt[:,1])
      mins,maxs=get_min_max_df(df,plt_names[name])
      for m in mins:
        plt.scatter(m[0],m[1],marker='x')
      for m in maxs:
        plt.scatter(m[0],m[1],marker='x')
    plt.title(name)
    plt.show()

def viz_specific_clusters(df,clus_names,clus_nums):
  n_cn={}
  for n in clus_nums:
    n_cn[n]=clus_names[n]
  
  rest=[]
  for k in clus_names:
    if k not in clus_nums:
      rest.append(clus_names[k])
  
  n_cn[11]=np.concatenate(rest)
  viz_clusters(df,n_cn)

viz_clusters(X_2d2,clus_names)

viz_specific_clusters(X_2d2,clus_names,[7,2])

|viz_specific_clusters(X_2d2,clus_names,[1,7])

# for one cluster (decision making intutoin)
plt_ranges_single_cluster(df_all_cl2,{1:clus_names[1],2:clus_names[7]})

def tsne(X):
  ts=TSNE(2)
  X_2d=ts.fit_transform(X)
  return X_2d

X_2d_dir=tsne(X_scaled)

viz_clusters(X_2d_dir,clus_names_dir)

viz_specific_clusters(X_2d_dir,clus_names_dir,[0,4])

plt_ranges_single_cluster(df_all_cl2,{2:clus_names_dir[2],3:clus_names_dir[1]})

